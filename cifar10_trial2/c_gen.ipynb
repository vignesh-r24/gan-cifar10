{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, LeakyReLU, ReLU, Flatten, Dense\n",
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets.cifar10 import load_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_disc_model():\n",
    "    # results in an output image size of 16x16x3\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same', input_shape=(32,32,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # results in an output image size of 8x8x3\n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # discriminator basically detects real/fake\n",
    "    # so binary crossentropy loss can be used\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "#     model.build(input_shape=(None, 28,28,3))\n",
    "#     print(model.summary())\n",
    "    return model\n",
    "\n",
    "def define_gen_model():\n",
    "    model = Sequential()\n",
    "    hidden_nodes = 128*8*8\n",
    "    input_nodes = 100\n",
    "    # input layer = 100 nodes, taken randomly from a Guassian distribution\n",
    "    # hidden layer = 128 * 8 * 8 nodes, representing 128 8*8 images\n",
    "    # each of these is a low-res / compressed version of the final image\n",
    "    model.add(Dense(hidden_nodes, input_dim=input_nodes))\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(ReLU())\n",
    "    model.add(Reshape((8,8,128)))\n",
    "    \n",
    "    # upsample: 128 filters of size 4x4, operating on every 8x8 image\n",
    "    # output image size 16x16\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    #output image size is 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    # Output image size is 32x32x3\n",
    "    model.add(Conv2D(3, (5,5), activation='tanh', padding='same'))\n",
    "    \n",
    "#     model.build(input_shape=(None, 100))\n",
    "#     print(model.summary())\n",
    "    return model\n",
    "\n",
    "def define_gan_model(d_model, g_model):\n",
    "    # freeze the discriminator model\n",
    "    # assert that all images received by d_model are real\n",
    "    # this will generate a loss, which will be used by g_model\n",
    "    # to improve the quality of generated images\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(g_model)\n",
    "    model.add(d_model)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#     model.build(input_shape=(None, 100))\n",
    "#     print(model.summary())\n",
    "    return model\n",
    "\n",
    "def load_real_samples():\n",
    "    (train_x, train_y), (test_x, test_y) = load_data()\n",
    "    \n",
    "    # scale the pixel values between -1 and 1\n",
    "    train_x = train_x.astype('float32')\n",
    "    train_x = ( train_x - 127.5 ) / 127.5\n",
    "    return train_x \n",
    "\n",
    "def gen_real_subset(train_x, n_samples):\n",
    "    chosen_indices = np.random.randint(0, train_x.shape[0], n_samples)\n",
    "    subset_x = train_x[chosen_indices]\n",
    "    subset_y = np.ones((n_samples, 1))\n",
    "    return subset_x, subset_y\n",
    "\n",
    "def gen_fake_subset(shape, n_samples):\n",
    "    subset_x = np.random.uniform(size=(n_samples, shape[0], shape[1], shape[2]))\n",
    "    subset_y = np.zeros((n_samples, 1))\n",
    "    return subset_x, subset_y\n",
    "\n",
    "def gen_latent_inputs(n_samples, dimension):\n",
    "    latent_input = np.random.randn(n_samples, dimension)\n",
    "    return latent_input\n",
    "\n",
    "def gen_fake_samples(g_model, n_samples, latent_dim):\n",
    "    latent_inputs = gen_latent_inputs(n_samples, latent_dim)\n",
    "    fake_x = g_model.predict(latent_inputs)\n",
    "    fake_y = np.zeros((n_samples, 1))\n",
    "    return fake_x, fake_y\n",
    "\n",
    "def train_d_model(d_model, train_x, batch_size=128, iterations=100):\n",
    "    n_samples = int(batch_size / 2)\n",
    "    for i in range(iterations):\n",
    "        real_x, real_y = gen_real_subset(train_x, n_samples)\n",
    "        img_shape = real_x.shape[1:]\n",
    "        fake_x, fake_y = gen_fake_subset(img_shape, n_samples)\n",
    "        \n",
    "        _, real_acc = d_model.train_on_batch(real_x, real_y)\n",
    "        _, fake_acc = d_model.train_on_batch(fake_x, fake_y)\n",
    "    \n",
    "    print(f'Real Accuracy: {real_acc*100:.2f}\\tFake Accuracy: {fake_acc*100:.2f}')\n",
    "\n",
    "    return d_model\n",
    "       \n",
    "def train_gan(d_model, g_model, gan_model, train_x, latent_dim=100, batch_size=256, n_epochs=100):\n",
    "    n_samples = int(batch_size / 2)\n",
    "    n_batches = int(train_x.shape[0] / batch_size)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(n_batches):\n",
    "            \n",
    "            real_x, real_y = gen_real_subset(train_x, n_samples)\n",
    "            d_loss1, d_acc1 = d_model.train_on_batch(real_x, real_y)\n",
    "            \n",
    "            fake_x, fake_y = gen_fake_samples(g_model, n_samples, latent_dim)\n",
    "            d_loss2, d_acc2 = d_model.train_on_batch(fake_x, fake_y)\n",
    "            \n",
    "#             d_x, d_y = np.vstack((real_x, fake_x)), np.vstack((real_y, fake_y))\n",
    "#             d_loss, d_acc = d_model.train_on_batch(d_x, d_y)\n",
    "                        \n",
    "            # gan will receive (n_samples * 2) latent space vectors\n",
    "            gan_x = gen_latent_inputs(n_samples*2, latent_dim)\n",
    "            # these latent inputs will be marked as real i.e. 1\n",
    "            gan_y = np.ones((n_samples*2, 1))\n",
    "                        \n",
    "            # during training, only generator weights will be updated,\n",
    "            # since we have frozen discriminator weights in gan definition\n",
    "            gan_loss, gan_acc = gan_model.train_on_batch(gan_x, gan_y)\n",
    "            \n",
    "            print(f'D Loss Real: {d_loss1:.4f}\\tD Loss Fake: {d_loss2:.4f}\\tGAN Loss: {gan_loss:.4f}\\tBatch: {batch+1}\\tEpoch: {epoch+1}')\n",
    "        \n",
    "        if((epoch+1) % 5 == 0):\n",
    "            evaluate_gan(d_model, g_model, train_x, latent_dim, epoch, n_samples)\n",
    "\n",
    "def evaluate_gan(d_model, g_model, train_x, latent_dim, n_epoch, n_samples=25):\n",
    "    # discriminator performance on real samples\n",
    "    real_x, real_y = gen_real_subset(train_x, n_samples)\n",
    "    real_loss, real_acc = d_model.evaluate(real_x, real_y)\n",
    "    \n",
    "    # discriminator perfomance on fake samples\n",
    "    fake_x, fake_y = gen_fake_samples(g_model, n_samples, latent_dim)\n",
    "    fake_loss, fake_acc = d_model.evaluate(fake_x, fake_y)\n",
    "    \n",
    "    print(f'Real Accuracy: {real_acc*100:.2f}\\tFake Accuracy: {fake_acc*100:.2f}')    \n",
    "    \n",
    "    # save the generator model snapshots\n",
    "    f_name = f'./gen_models/g_model_e_{n_epoch+1}.h5'\n",
    "    g_model.save(f_name)\n",
    "    \n",
    "    # plot generated images, save to file\n",
    "    save_plot(fake_x[0:4], n_epoch)\n",
    "\n",
    "def save_plot(images, n_epoch):\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.axis('off')\n",
    "        # scale images back to [0,1] before display\n",
    "        plt.imshow((127.5*(1+images[i])).astype(np.uint8))\n",
    "    f_name = f'./gen_images/image_e_{n_epoch+1}.png'\n",
    "    plt.savefig(f_name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminative model\n",
    "d_model = define_disc_model()\n",
    "g_model = define_gen_model()\n",
    "gan_model = define_gan_model(d_model, g_model)\n",
    "train_x = load_real_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=500\n",
    "batch_size = 128\n",
    "iterations = 50\n",
    "latent_dim = 100\n",
    "n_fakes = 25\n",
    "# d_model = train_d_model(d_model, train_x, batch_size=batch_size, iterations=iterations)\n",
    "# real_x, real_y = gen_real_subset(train_x, n_fakes)\n",
    "# fake_x, fake_y = gen_fake_samples(g_model, n_fakes, latent_dim)\n",
    "# for i in range(n_fakes):\n",
    "#     plt.subplot(5, 5, 1+i)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(fake_x[i, :, :,:])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 128\n",
    "# gan_x = gen_latent_inputs(n_samples*2, latent_dim)\n",
    "# gan_y = np.ones((n_samples*2, 1))\n",
    "# gan_x.shape, gan_y.shape\n",
    "# d_model.predict(fake_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gan(d_model, g_model, gan_model, train_x, latent_dim, batch_size, n_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcgan_py37",
   "language": "python",
   "name": "dcgan_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
