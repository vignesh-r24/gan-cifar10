{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, LeakyReLU, ReLU, Flatten, Dense\n",
    "from keras.layers import Reshape, Conv2DTranspose, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.datasets.cifar10 import load_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_disc_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), padding='same', input_shape=(32,32,3)))\n",
    "    \n",
    "    # output image size of 16x16x3\n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    # output image size of 8x8x3\n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # discriminator basically detects real/fake\n",
    "    # so binary crossentropy loss can be used\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "#     model.build(input_shape=(None, 32,32,3))\n",
    "#     print(model.summary())\n",
    "    return model\n",
    "\n",
    "def define_gen_model():\n",
    "    model = Sequential()\n",
    "    hidden_nodes = 256*8*8\n",
    "    input_nodes = 100\n",
    "    kernel_init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "    # input layer = 100 nodes, taken randomly from a Guassian distribution\n",
    "    # hidden layer = 128 * 8 * 8 nodes, representing 128 4*4 images\n",
    "    # each of these is a low-res / compressed version of the final image\n",
    "    model.add(Dense(hidden_nodes, input_dim=input_nodes))\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(ReLU())\n",
    "    model.add(Reshape((8,8,256)))\n",
    "    \n",
    "    # output image size 16x16\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=kernel_init))\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    # output image size is 32x32\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=kernel_init))\n",
    "    model.add(BatchNormalization())\n",
    "#     model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    model.add(Conv2D(3, (4,4), activation='tanh', padding='same', kernel_initializer=kernel_init))\n",
    "    \n",
    "#     modecompileuild(input_shape=(None, 100))\n",
    "#     print(model.summary())\n",
    "    return model\n",
    "\n",
    "def define_gan_model(d_model, g_model):\n",
    "    # freeze the discriminator model\n",
    "    # assert that all images received by d_model are real\n",
    "    # this will generate a loss, which will be used by g_model\n",
    "    # to improve the quality of generated images\n",
    "    d_model.trainable = False\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(g_model)\n",
    "    model.add(d_model)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#     model.build(input_shape=(None, 100))\n",
    "#     print(model.summary())\n",
    "    return model\n",
    "\n",
    "def load_real_samples():\n",
    "    (train_x, train_y), (test_x, test_y) = load_data()\n",
    "    \n",
    "    # scale the pixel values between -1 and 1\n",
    "    train_x = train_x.astype('float32')\n",
    "    train_x = ( train_x - 127.5 ) / 127.5\n",
    "    return train_x \n",
    "\n",
    "def gen_real_subset(train_x, n_samples, smoothing=False):\n",
    "    chosen_indices = np.random.randint(0, train_x.shape[0], n_samples)\n",
    "    subset_x = train_x[chosen_indices]\n",
    "    if not smoothing:\n",
    "        subset_y = np.ones((n_samples, 1))\n",
    "    else:\n",
    "        # instead of 1, assign a random value between 0.8 and 1.2\n",
    "        subset_y = np.empty((n_samples,1))\n",
    "        for i in range(n_samples):\n",
    "            subset_y[i] = round(np.random.uniform(0.8,1.2), 1)\n",
    "    return subset_x, subset_y\n",
    "\n",
    "def gen_fake_subset(shape, n_samples):\n",
    "    subset_x = np.random.uniform(size=(n_samples, shape[0], shape[1], shape[2]))\n",
    "    subset_y = np.zeros((n_samples, 1))\n",
    "    return subset_x, subset_y\n",
    "\n",
    "def gen_latent_inputs(n_samples, dimension):\n",
    "    latent_input = np.random.randn(n_samples, dimension)\n",
    "    return latent_input\n",
    "\n",
    "def gen_fake_samples(g_model, n_samples, latent_dim, smoothing=False):\n",
    "    latent_inputs = gen_latent_inputs(n_samples, latent_dim)\n",
    "    fake_x = g_model.predict(latent_inputs)\n",
    "    if not smoothing:\n",
    "        fake_y = np.zeros((n_samples, 1))\n",
    "    else:\n",
    "        # instead of 0, assign a random value between 0 and 0.3\n",
    "        fake_y = np.empty((n_samples, 1))\n",
    "        for i in range(n_samples):\n",
    "            fake_y[i] = round(np.random.uniform(0,0.3), 1)\n",
    "    return fake_x, fake_y\n",
    "\n",
    "def train_d_model(d_model, train_x, batch_size=128, iterations=100):\n",
    "    n_samples = int(batch_size / 2)\n",
    "    for i in range(iterations):\n",
    "        real_x, real_y = gen_real_subset(train_x, n_samples)\n",
    "        img_shape = real_x.shape[1:]\n",
    "        fake_x, fake_y = gen_fake_subset(img_shape, n_samples)\n",
    "        \n",
    "        _, real_acc = d_model.train_on_batch(real_x, real_y)\n",
    "        _, fake_acc = d_model.train_on_batch(fake_x, fake_y)\n",
    "    \n",
    "    print(f'Real Accuracy: {real_acc*100:.2f}\\tFake Accuracy: {fake_acc*100:.2f}')\n",
    "\n",
    "    return d_model\n",
    "       \n",
    "def train_gan(d_model, g_model, gan_model, train_x, latent_dim=100, batch_size=256, n_epochs=100):\n",
    "    n_samples = int(batch_size / 2)\n",
    "    n_batches = int(train_x.shape[0] / batch_size)\n",
    "\n",
    "    with open('train_history.log', 'a+') as train_log:\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch in range(n_batches):\n",
    "\n",
    "                real_x, real_y = gen_real_subset(train_x, n_samples, smoothing=True)\n",
    "                d_loss1, d_acc1 = d_model.train_on_batch(real_x, real_y)\n",
    "\n",
    "                fake_x, fake_y = gen_fake_samples(g_model, n_samples, latent_dim, smoothing=True)\n",
    "                d_loss2, d_acc2 = d_model.train_on_batch(fake_x, fake_y)\n",
    "\n",
    "                # gan will receive (n_samples * 2) latent space vectors\n",
    "                gan_x = gen_latent_inputs(n_samples*2, latent_dim)\n",
    "                # these latent inputs will be marked as real i.e. 1\n",
    "                gan_y = np.ones((n_samples*2, 1))\n",
    "\n",
    "                # during training, only generator weights will be updated,\n",
    "                # since we have frozen discriminator weights in gan definition\n",
    "                gan_loss, gan_acc = gan_model.train_on_batch(gan_x, gan_y)\n",
    "\n",
    "                metric = f'D Loss Real: {d_loss1:.4f}\\tD Loss Fake: {d_loss2:.4f}\\tGAN Loss: {gan_loss:.4f}\\tBatch: {batch+1}\\tEpoch: {epoch+1}'\n",
    "                train_log.write(metric+'\\n')\n",
    "\n",
    "            if((epoch+1) % 10 == 0):\n",
    "                evaluate_gan(d_model, g_model, train_x, latent_dim, epoch, n_samples)\n",
    "\n",
    "def evaluate_gan(d_model, g_model, train_x, latent_dim, n_epoch, n_samples=25):\n",
    "    # discriminator performance on real samples\n",
    "    real_x, real_y = gen_real_subset(train_x, n_samples, smoothing=True)\n",
    "    real_loss, real_acc = d_model.evaluate(real_x, real_y)\n",
    "    \n",
    "    # discriminator perfomance on fake samples\n",
    "    fake_x, fake_y = gen_fake_samples(g_model, n_samples, latent_dim, smoothing=True)\n",
    "    fake_loss, fake_acc = d_model.evaluate(fake_x, fake_y)\n",
    "    \n",
    "    metric = f'R Acc: {real_acc*100:.2f}\\tR Loss: {real_loss:.4f}\\tF Acc: {fake_acc*100:.2f}\\tF Loss: {fake_loss:.4f}\\tEpoch: {n_epoch+1}'\n",
    "    with open('test_history.log', 'a+') as test_log:\n",
    "        test_log.write(metric+'\\n')\n",
    "    \n",
    "    # save the generator model snapshots\n",
    "    f_name = f'./gen_models/g_model_e_{n_epoch+1}.h5'\n",
    "    g_model.save(f_name)\n",
    "    \n",
    "    # plot generated images, save to file\n",
    "    save_plot(fake_x[0:10], n_epoch)\n",
    "\n",
    "def save_plot(images, n_epoch):\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow((127.5*(1+images[i])).astype(np.uint8))\n",
    "    f_name = f'./gen_images/image_e_{n_epoch+1}.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f_name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Format: channels_last\n"
     ]
    }
   ],
   "source": [
    "print(f'Image Data Format: {backend.image_data_format()}')\n",
    "d_model = define_disc_model()\n",
    "g_model = define_gen_model()\n",
    "gan_model = define_gan_model(d_model, g_model)\n",
    "train_x = load_real_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=200\n",
    "batch_size = 128\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_gan(d_model, g_model, gan_model, train_x, latent_dim, batch_size, n_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
